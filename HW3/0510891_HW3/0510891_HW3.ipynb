{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3: Decision Tree and Random Forest\n",
    "In hw3, you need to implement decision tree and random forest by using only numpy, then train your implemented model by the provided dataset and test the performance with testing data\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /opt/anaconda3/lib/python3.7/site-packages (0.0)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.7/site-packages (from sklearn) (0.21.3)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.13.2)\r\n",
      "Requirement already satisfied: numpy>=1.11.0 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.17.2)\r\n",
      "Requirement already satisfied: scipy>=0.17.0 in /opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.3.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "feature_names = data['feature_names']\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\"x_train.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\")\n",
    "x_test = pd.read_csv(\"x_test.csv\")\n",
    "y_test = pd.read_csv(\"y_test.csv\")\n",
    "xy_train = pd.concat([x_train, y_train], axis=1)\n",
    "xy_test = pd.concat([x_test, y_test], axis=1)\n",
    "raw_xy_train = xy_train.values.tolist()\n",
    "raw_xy_test = xy_train.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from page 666 on the textbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(sequence):\n",
    "    num_of_class_one = 0\n",
    "    size = len(sequence)\n",
    "    if size == 0:\n",
    "        return 0\n",
    "    for i in sequence:\n",
    "        if i == 1:\n",
    "            num_of_class_one += 1\n",
    "    proportion = num_of_class_one / size\n",
    "    return 1 - proportion ** 2 - (1 - proportion) ** 2\n",
    "\n",
    "def entropy(sequence):\n",
    "    num_of_class_one = 0\n",
    "    size = len(sequence)\n",
    "    if size == 0:\n",
    "        return 0\n",
    "    for i in sequence:\n",
    "        if i == 1:\n",
    "            num_of_class_one += 1\n",
    "    proportion = num_of_class_one / size\n",
    "    if proportion == 0:\n",
    "        return 0\n",
    "    return - proportion * math.log(proportion, 2) - proportion * math.log(proportion, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = class 1,\n",
    "# 2 = class 2\n",
    "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of data is  0.8299157956468823\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the test data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **Criterion**: The function to measure the quality of a split. Your model should support “gini” for the Gini impurity and “entropy” for the information gain. \n",
    "2. **Max_depth**: The maximum depth of the tree. If Max_depth=None, then nodes are expanded until all leaves are pure. Max_depth=1 equals to split data once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.root = None\n",
    "        self.feature_importance = None\n",
    "        \n",
    "    def partition(self, index, value, dataset):\n",
    "        left, right = list(), list()\n",
    "        for row in dataset:\n",
    "            if row[index] < value:\n",
    "                left.append(row)\n",
    "            else:\n",
    "                right.append(row)\n",
    "        return left, right\n",
    "    \n",
    "    def total_impurity(self, groups):\n",
    "        g, y = [], []\n",
    "        for group in groups:\n",
    "            for i in range(len(group)):\n",
    "                y.append(group[i][-1] + 1)\n",
    "            if self.criterion == 'gini':\n",
    "                g.append(gini(np.array(y)))\n",
    "            else:\n",
    "                g.append(entropy(np.array(y)))\n",
    "        p = len(groups[0]) / (len(groups[0]) + len(groups[1]))\n",
    "        return p * g[0] + (1 - p) * g[1]\n",
    "\n",
    "    def find_best_split(self, dataset):\n",
    "        best_feature, best_compare_num, lowest_impurity, best_groups = 0, 0, 999, None\n",
    "        for feature in range(len(dataset[0]) - 1):\n",
    "            for row in dataset:\n",
    "                groups = self.partition(feature, row[feature], dataset)\n",
    "                impurity = self.total_impurity(groups)\n",
    "                if impurity < lowest_impurity:\n",
    "                    best_feature, best_compare_num, lowest_impurity, best_groups = feature, row[feature], impurity, groups\n",
    "        self.feature_importance[best_feature] += self.node_fi(dataset, best_groups)\n",
    "        return {'feature': best_feature, 'value': best_compare_num, 'impurity': lowest_impurity, 'groups': best_groups}\n",
    "    \n",
    "    def leaf(self, group):\n",
    "        classes = [row[-1] for row in group]\n",
    "        return max(set(classes), key=classes.count)\n",
    "    \n",
    "    def split(self, node, depth):\n",
    "        left, right = node['groups']\n",
    "        del(node['groups'])\n",
    "        if not left or not right:\n",
    "            node['left'] = node['right'] = self.leaf(left + right)\n",
    "            return\n",
    "        if depth >= self.max_depth:\n",
    "            node['left'], node['right'] = self.leaf(left), self.leaf(right)\n",
    "            return\n",
    "        node['left'] = self.find_best_split(left)\n",
    "        self.split(node['left'], depth+1)\n",
    "        node['right'] = self.find_best_split(right)\n",
    "        self.split(node['right'], depth+1)\n",
    "\n",
    "    def build_tree(self, dataset):\n",
    "        self.feature_importance = np.zeros(len(dataset[0]) - 1)\n",
    "        self.root = self.find_best_split(dataset)\n",
    "        self.split(self.root, 1)\n",
    "        return self.root\n",
    "\n",
    "    def print_tree(self, node, depth=0):\n",
    "        if isinstance(node, dict):\n",
    "            print('%s[%s < %.3f]' % ((depth*' ', (feature_names[node['feature']]), node['value'])))\n",
    "            self.print_tree(node['left'], depth+1)\n",
    "            self.print_tree(node['right'], depth+1)\n",
    "        else:\n",
    "            print('%s[%s]' % ((depth*' ', node)))\n",
    "\n",
    "    def classify(self, node, row):\n",
    "        if row[node['feature']] < node['value']:\n",
    "            if isinstance(node['left'], dict):\n",
    "                return self.classify(node['left'], row)\n",
    "            else:\n",
    "                return node['left']\n",
    "        else:\n",
    "            if isinstance(node['right'], dict):\n",
    "                return self.classify(node['right'], row)\n",
    "            else:\n",
    "                return node['right']\n",
    "            \n",
    "    def accuracy(self, dataset):\n",
    "        correct = 0\n",
    "        for row in dataset:\n",
    "            result = self.classify(self.root, row)\n",
    "            if result == row[-1]:\n",
    "                correct += 1\n",
    "        return correct / len(dataset)\n",
    "    \n",
    "    def node_fi(self, dataset, groups):\n",
    "        father = gini([row[-1] for row in dataset]) * len(dataset)\n",
    "        son1 = gini([row[-1] for row in groups[0]]) * len(groups[0])\n",
    "        son2 = gini([row[-1] for row in groups[1]]) * len(groups[1])\n",
    "        return father - son1 - son2\n",
    "    \n",
    "    def plot_fi(self):\n",
    "        fi_dict = {k: self.feature_importance[v] for v, k in enumerate(feature_names)}\n",
    "        fi_sorted_list = [i for i in (sorted(fi_dict.items(), key=lambda x:x[1])) if i[1] != 0]\n",
    "        plt.barh(np.arange(len(fi_sorted_list)), [i[1] for i in fi_sorted_list])\n",
    "        plt.yticks(np.arange(len(fi_sorted_list)), [i[0] for i in fi_sorted_list])\n",
    "        plt.title(\"Feature Importance\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using Criterion=‘gini’, showing the accuracy score of test data by Max_depth=3 and Max_depth=10, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9178403755868545\n",
      "accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_depth3.build_tree(raw_xy_train)\n",
    "print(\"accuracy: \", clf_depth3.accuracy(raw_xy_test))\n",
    "\n",
    "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
    "clf_depth10.build_tree(raw_xy_train)\n",
    "print(\"accuracy: \", clf_depth10.accuracy(raw_xy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using Max_depth=3, showing the accuracy score of test data by Criterion=‘gini’ and Criterion=’entropy’, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9178403755868545\n",
      "accuracy:  0.9178403755868545\n"
     ]
    }
   ],
   "source": [
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_gini.build_tree(raw_xy_train)\n",
    "print(\"accuracy: \", clf_gini.accuracy(raw_xy_test))\n",
    "\n",
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)\n",
    "clf_entropy.build_tree(raw_xy_train)\n",
    "print(\"accuracy: \", clf_entropy.accuracy(raw_xy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: All of your accuracy scores should over 0.9\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the feature counts for building tree without normalize the importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_gini.build_tree(raw_xy_test)\n",
    "clf_gini.plot_fi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement two arguments for the Random Forest.\n",
    "\n",
    "1. **N_estimators**: The number of trees in the forest. \n",
    "2. **Max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **Bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(DecisionTree):\n",
    "    def __init__(self, n_estimators, max_features, boostrap=True, criterion='gini', max_depth=3):\n",
    "        DecisionTree.__init__(self, criterion, max_depth)\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = math.floor(max_features)\n",
    "        self.forest = None\n",
    "    \n",
    "    def find_best_split(self, dataset):\n",
    "        best_feature, best_compare_num, lowest_impurity, best_groups = 0, 0, 999, None\n",
    "        select_feature_list = random.sample(list(range(len(feature_names))), max_features)\n",
    "        for feature in select_feature_list:\n",
    "            for row in dataset:\n",
    "                groups = self.partition(feature, row[feature], dataset)\n",
    "                impurity = self.total_impurity(groups)\n",
    "                if impurity < lowest_impurity:\n",
    "                    best_feature, best_compare_num, lowest_impurity, best_groups = feature, row[feature], impurity, groups\n",
    "        self.feature_importance[best_feature] += self.node_fi(dataset, best_groups)\n",
    "        return {'feature': best_feature, 'value': best_compare_num, 'impurity': lowest_impurity, 'groups': best_groups}\n",
    "    \n",
    "    def build_forest(self, dataset):\n",
    "        trees = []\n",
    "        data_list = self.split_data(dataset)\n",
    "        for i in range(self.n_estimators):\n",
    "            clf = DecisionTree(self.criterion, self.max_depth)\n",
    "            trees.append(clf.build_tree(data_list[i]))\n",
    "        self.forest = trees\n",
    "        return trees\n",
    "    \n",
    "    def print_forest(self, forest, depth=0):\n",
    "        for i in range(self.n_estimators):\n",
    "            self.print_tree(forest[i], depth=0)\n",
    "        \n",
    "    def split_data(self, dataset):\n",
    "        data_list = list()\n",
    "        for i in range(self.n_estimators):\n",
    "            tmp_list = list()\n",
    "            sample_amount = int(len(dataset) * 2 / 3)\n",
    "            slist = random.sample(list(range(len(dataset))), sample_amount)\n",
    "            for j in range(sample_amount):\n",
    "                tmp_list.append(dataset[slist[j]])\n",
    "            data_list.append(tmp_list)\n",
    "        return data_list\n",
    "    \n",
    "    def accuracy(self, dataset):\n",
    "        correct = 0\n",
    "        for row in dataset:\n",
    "            outcome = []\n",
    "            for tree in self.forest:\n",
    "                outcome.append(self.classify(tree, row))\n",
    "            result = max(set(outcome), key=outcome.count)\n",
    "            if result == row[-1]:\n",
    "                correct += 1\n",
    "        return correct / len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Using Criterion=‘gini’, Max_depth=None, Max_features=sqrt(n_features), showing the accuracy score of test data by n_estimators=10 and n_estimators=100, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9436619718309859\n",
      "accuracy:  0.9436619718309859\n"
     ]
    }
   ],
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=math.sqrt(x_train.shape[1]))\n",
    "clf_10tree.build_forest(raw_xy_train)\n",
    "print(\"accuracy: \", clf_10tree.accuracy(raw_xy_test))\n",
    "clf_100tree = RandomForest(n_estimators=100, max_features=math.sqrt(x_train.shape[1]))\n",
    "clf_100tree.build_forest(raw_xy_train)\n",
    "print(\"accuracy: \", clf_10tree.accuracy(raw_xy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2\n",
    "Using Criterion=‘gini’, Max_depth=None, N_estimators=10, showing the accuracy score of test data by Max_features=sqrt(n_features) and Max_features=n_features, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9436619718309859\n",
      "accuracy:  0.9366197183098591\n"
     ]
    }
   ],
   "source": [
    "clf_all_features = RandomForest(n_estimators=10, max_features=math.sqrt(x_train.shape[1]))\n",
    "clf_all_features.build_forest(raw_xy_train)\n",
    "print(\"accuracy: \", clf_all_features.accuracy(raw_xy_test))\n",
    "clf_random_features = RandomForest(n_estimators=10, max_features=x_train.shape[1])\n",
    "clf_random_features.build_forest(raw_xy_train)\n",
    "print(\"accuracy: \", clf_random_features.accuracy(raw_xy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary\n",
    "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
